{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc3b0862-d89b-497d-890c-3e73f3b2d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 0: Import libs, create functions\n",
    "# We will need the RBCPath type from the rbclib package to load data from the RBC.\n",
    "from rbclib import RBCPath\n",
    "\n",
    "# We'll also want to load some data directly from the filesystem.\n",
    "from pathlib import Path\n",
    "\n",
    "# We'll want to load/process some of the data using pandas and numpy.\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f8be722-2861-44b9-8edf-208366d623ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fsdata(participant_id, local_cache_dir=(Path.home() / 'cache')):\n",
    "    \"Loads and returns the dataframe of a PNC participant's FreeSurfer data.\"\n",
    "\n",
    "    # Check that the local_cache_dir exists and make it if it doesn't.\n",
    "    if local_cache_dir is not None:\n",
    "        local_cache_dir = Path(local_cache_dir)\n",
    "        local_cache_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Make the RBCPath and find the appropriate file:\n",
    "    pnc_freesurfer_path = RBCPath(\n",
    "        'rbc://PNC_FreeSurfer/freesurfer',\n",
    "        # We provide the local_cache_dir to the RBCPath object; all paths made\n",
    "        # from this object will use the same cache directory.\n",
    "        local_cache_dir=local_cache_dir)\n",
    "    participant_path = pnc_freesurfer_path / f'sub-{participant_id}'\n",
    "    tsv_path = participant_path / f'sub-{participant_id}_regionsurfacestats.tsv'\n",
    "\n",
    "    # Use pandas to read in the TSV file:\n",
    "    with tsv_path.open('r') as f:\n",
    "        data = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "    # Return the loaded data:\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23fe2734-11f7-424b-9946-58dda5b33a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_acc_corthick(participant_id):\n",
    "    \"Loads and returns the bilateral Anterior Cingulate Cortex (ACC) average thickness for a PNC study participant.\"\n",
    "    \n",
    "    # First, load the subject's FreeSurfer dataframe:\n",
    "    data = load_fsdata(participant_id)\n",
    "    # Next, find the relevant rows:\n",
    "    row_mask = ((data['StructName'] == 'Brodmann.24') | (data['StructName'] == 'Brodmann.32') | (data['StructName'] == 'Brodmann.33'))\n",
    "    # Then extract and sum the surface areas:\n",
    "    acc_corthick = data.loc[row_mask, 'ThickAvg']\n",
    "    acc_corthick_mean = np.mean(acc_corthick)\n",
    "    # And return this value:\n",
    "    return acc_corthick_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe5a84e5-96f3-4985-b703-00f579d28ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.048"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_participant_id = 1000393599\n",
    "test = load_acc_corthick(example_participant_id)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaac10d2-47c4-479f-9799-1b84bc9a734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participant meta-data is generally located in the BIDS repository for each\n",
    "# study:\n",
    "rbcdata_path = Path('/home/jovyan/shared/data/RBC')\n",
    "train_filepath = rbcdata_path / 'train_participants.tsv'\n",
    "test_filepath = rbcdata_path / 'test_participants.tsv'\n",
    "\n",
    "# Load the PNC participants TSV files...\n",
    "with train_filepath.open('r') as f:\n",
    "    train_data = pd.read_csv(f, sep='\\t')\n",
    "with test_filepath.open('r') as f:\n",
    "    test_data = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "# We can also concatenate the two datasets into a single dataset of all\n",
    "# study participants:\n",
    "all_data = pd.concat([train_data, test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ad859-9c76-46aa-8417-9323343a0d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading surface areas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4678fff962a4ebdbf89385ea8eba291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1601)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First load in surface area data for each participant:\n",
    "print(\"Loading surface areas...\")     \n",
    "\n",
    "# We will put the rows in this dictionary of lists as we build the dataframe:\n",
    "all_vars = {\n",
    "    'participant_id': [],\n",
    "    'acc_cor_thick': [],\n",
    "    'p_factor': []}\n",
    "\n",
    "# We'll display a progress bar `prog` as we go also:\n",
    "from ipywidgets import IntProgress\n",
    "prog = IntProgress(min=0, max=len(all_data))\n",
    "display(prog)\n",
    "\n",
    "# Okay, loop through each row of the `all_data` dataframe, which contains both\n",
    "# training and test subjects, load their BA1 data, and store it in the\n",
    "# all_vars dictionary.\n",
    "for (ii, row) in all_data.iterrows():\n",
    "    # Extract the participant ID and p_factor (which will be NaN for test\n",
    "    # participants).\n",
    "    participant_id = row['participant_id']\n",
    "    p_factor = row['p_factor']\n",
    "    \n",
    "    # Load the surface area for this participant:\n",
    "    try:\n",
    "        acc_cor_thick = load_acc_corthick(participant_id)\n",
    "    except FileNotFoundError:\n",
    "        # Some subjects are just missing the file, so we code them as NaN.\n",
    "        surf_area = np.nan\n",
    "    \n",
    "    # Append the participant ID and their surface area to our dataset:\n",
    "    all_vars['participant_id'].append(participant_id)\n",
    "    all_vars['acc_cor_thick'].append(acc_cor_thick)\n",
    "    all_vars['p_factor'].append(p_factor)\n",
    "    # Increment the progress bar counter:\n",
    "    prog.value += 1\n",
    "\n",
    "# Convert train_vars into a dataframe.\n",
    "all_vars = pd.DataFrame(all_vars)\n",
    "\n",
    "# Extract the training and test subjects into separate dataframes; the test\n",
    "# participants can be identified as those having NaN values for their\n",
    "# p_factor column.\n",
    "train_vars = all_vars[~np.isnan(all_vars['p_factor'])]\n",
    "test_vars = all_vars[np.isnan(all_vars['p_factor'])]\n",
    "\n",
    "# Display the finished dataframe.\n",
    "all_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942cf3d8-2cb3-4487-b36d-82b797ca6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LinearRegression type:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# LinearRegression requires a matrix whose columns are the variables and whose\n",
    "# final column is the value being predicted (the p_factor for us). We can\n",
    "# extract these columns straight from the dataframes we generated.\n",
    "train_matrix = train_vars.loc[:, ['acc_cor_thick', 'p_factor']].values\n",
    "# We need to exclude rows with NaNs for training:\n",
    "train_okrows = np.all(~np.isnan(train_matrix), axis=1)\n",
    "train_matrix = train_matrix[train_okrows]\n",
    "\n",
    "# Train the regression using the training matrix:\n",
    "lreg = LinearRegression()\n",
    "lreg.fit(train_matrix[:, :1], train_matrix[:, 1])\n",
    "\n",
    "# Display the trained regression parameters:\n",
    "print(\"Linear Regression:\")\n",
    "print(\"  Intercept:\", lreg.intercept_)\n",
    "print(\"  Slope:\", lreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c095d1-51a2-43c3-ab8d-51f8e67f99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can apply the trained linear regression object `lreg` to the 1-column\n",
    "# matrix of ba1_surface_area values in the test_vars dataframe.\n",
    "test_matrix = test_vars.loc[:, ['acc_cor_thick']].values\n",
    "test_okrows = np.all(~np.isnan(test_matrix), axis=1)\n",
    "test_matrix = test_matrix[test_okrows]\n",
    "\n",
    "# Apply the model:\n",
    "p_factor_predictions = lreg.predict(test_matrix)\n",
    "\n",
    "# Display the predictions:\n",
    "p_factor_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71dc583-234d-4955-b32b-4d8175aee8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.loc[test_okrows, 'p_factor'] = p_factor_predictions\n",
    "\n",
    "# Display the resulting test data:\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c696f0d-d995-40dc-baa8-90b187da9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sep='\\t' option here is necessary for tab-separated-value (as opposed to\n",
    "# comma-separated-value) files. The `index=False` just indicates that pandas\n",
    "# doesn't need to write out its own index column.\n",
    "\n",
    "group_name = 'group7'  # Change this to be your group name!\n",
    "\n",
    "test_data.to_csv(f'results/{group_name}.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
